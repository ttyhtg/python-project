Hello world!	http://www.santostang.com/2018/07/04/hello-world/	Welcome to WordPress. This is your first post. Edit or delete it, then start writing! 各位读者，由于网易云跟帖在本书出版后已经停止服务，书中的第四章已经无法使用。所以我将本书的评论系统换成了来必力。现在已经在博客和知乎上更新了新写的第四章。请查阅： 4.1 动态网页抓取 (解析真实地址 + selenium) 4.2 解析真实地址抓取 4.3 通过selenium 模拟浏览器抓取

4.2 解析真实地址抓取	http://www.santostang.com/2018/07/14/4-2-%e8%a7%a3%e6%9e%90%e7%9c%9f%e5%ae%9e%e5%9c%b0%e5%9d%80%e6%8a%93%e5%8f%96/	由于网易云跟帖停止服务，现在已经在此处中更新了新写的第四章。请参照文章： 4.2 解析真实地址抓取 虽然数据并没有出现在网页源代码中，我们也可以找到数据的真实地址，请求这个真实地址也可以获得想要的数据。这里用到浏览器的“检查”功能。 下面以我博客的“Hello World”文章为例，目标是抓取文章下的所有评论。文章网址为: http://www.santostang.com/2018/07/04/hello-world/ 步骤一：打开“检查”功能。用Chrome浏览器打开“Hello World”文章。右键页面任意位置，在弹出的对话框中，点击“检查”选项。得到如下图所示的对话框。  步骤二：找到真实的数据地址。点击对话框中的Network，然后刷新网页。此时，Network 会显示浏览器从网页服务器中得到的所有文件，一般这个过程称之为“抓包”。因为所有文件已经显示出来，所以我们需要的评论数据一定在其中。 一般而言，这些数据可能以 json 文件格式获取。所以，我们可以在Network中的 All找到真正的评论文件“list?callback=jQuery11240879907919223679”，其地址为： “https://api-zero.livere.com/v1/comments/list?callback=jQuery112403473268296510956_1531502963311&limit=10&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&_=1531502963313” 。点击 Preview 即可查看数据。  步骤三：爬取真实评论数据地址。既然找到了真实的地址，我们就可以直接用requests请求这个地址，获取数据。代码如下如所示： import requests  link = """https://api-zero.livere.com/v1/comments/list?callback=jQuery112403473268296510956_1531502963311&limit=10&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&_=1531502963313""" headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}  r = requests.get(link, headers= headers) print (r.text)  运行上述代码获得的结果为：  综上所述，如果要爬取类似淘宝网评论这种用AJAX加载的网页的话，从网页源代码中是找不到想要的数据的。需要用浏览器的审查元素，找到真实的数据地址。然后爬取真实的网站。 步骤三：从 json数据中提取评论。上述的结果比较杂乱，但是它其实是 json 数据，我们可以使用 json 库解析数据，从中提取我们想要的数据。 import json # 获取 json 的 string json_string = r.text json_string = json_string[json_string.find('{'):-2]  json_data = json.loads(json_string) comment_list = json_data['results']['parents']  for eachone in comment_list:     message = eachone['content']     print (message)  首先，我们需要使用json_string[json_string.find(‘{‘}:-2)]， 仅仅提取字符串中符合json格式的部分。然后，使用 json.loads 可以把字符串格式的响应体数据转化为 json 数据。然后，利用 json 数据的结构，我们可以提取到评论的列表comment_list。最后再通过一个 for 循环，提取其中的评论文本，并输出打印。 输出的结果为：  上述的教学，只是爬取文章的第一页评论，十分简单。其实，我们经常需要爬取所有页面，如果我们还是用人工一页页地翻页，找到评论数据的地址，就十分费力了。因此，下面将介绍网页 URL地址的规律，并用for循环爬取，就会非常轻松。 例如：刚刚的文章第一页评论的真实地址是 https://api-zero.livere.com/v1/comments/list?callback=jQuery112403473268296510956_1531502963311&limit=10&offset=1&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&_=1531502963316 如果我们继续点击“查看更多”，从“审查元素”中可以发现第二页的地址： https://api-zero.livere.com/v1/comments/list?callback=jQuery112403473268296510956_1531502963311&limit=10&offset=2&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&_=1531502963316 如果我们对比上面的两个地址，可以发现 URL 地址中有两个特别重要的变量，offset 和 limit，稍微理解一下可以知道，limit 代表的是每一页评论数量的最大值，也就是说，这里每一页评论最多显示30条；offset 代表的是第几页，第一页 offset 为0，第二页为1，那么第三页 offset 会是3。 因此，我们只需在URL中改变 offset 的值，便可以实现换页。以下是实现的代码： import requests import json  def single_page_comment(link):     headers = {'User-Agent' : 'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}      r = requests.get(link, headers= headers)     # 获取 json 的 string     json_string = r.text     json_string = json_string[json_string.find('{'):-2]     json_data = json.loads(json_string)     comment_list = json_data['results']['parents']  for eachone in comment_list:     message = eachone['content']      print (message)  for page in range(1,4):     link1 = "https://api-zero.livere.com/v1/comments/list?callback=jQuery112403473268296510956_1531502963311&limit=10&offset="     link2 = "&repSeq=4272904&requestPath=%2Fv1%2Fcomments%2Flist&consumerSeq=1020&livereSeq=28583&smartloginSeq=5154&_=1531502963316"     page_str = str(page)     link = link1 + page_str + link2     print (link)     single_page_comment(link)  在上述代码，函数single_page_comment(link)是之前爬取一个评论页面的代码，现在放入函数中，方便多次调取。另外，我们使用一个 for 循环，分别抓取第一页和第二页，在生成最终真实的URL地址后，调用函数抓取。 运行完代码，得到的结果是：  第四章其他章节请查看 第四章：动态网页抓取 (解析真实地址 + selenium) 4.2 解析真实地址抓取 4.3 通过selenium 模拟浏览器抓取

第四章- 动态网页抓取 (解析真实地址 + selenium)	http://www.santostang.com/2018/07/14/%e7%ac%ac%e5%9b%9b%e7%ab%a0%ef%bc%9a%e5%8a%a8%e6%80%81%e7%bd%91%e9%a1%b5%e6%8a%93%e5%8f%96-%e8%a7%a3%e6%9e%90%e7%9c%9f%e5%ae%9e%e5%9c%b0%e5%9d%80-selenium/	由于网易云跟帖停止服务，现在已经在此处中更新了新写的第四章。请参照文章： 前面爬取的网页均为静态网页，这样的网页在浏览器中展示的内容都在 HTML 源代码中。但是，由于主流网站都使用JavaScript 展现网页内容， 和静态网页不同的是，在使用JavaScript 时，很多内容并不会出现在 HTML 源代码中，所以爬取静态网页的技术可能无法正常使用。因此，我们 需要用到动态网页抓取的两种技术:通过浏览器审查元素解析真实网页地址 和使用 selenium 模拟浏览器的方法。 本章首先介绍动态网页的实例，让读者了解什么是动态抓取，然后使用上述两种动态网页抓取技术获取动态网页的数据。 4.1 动态抓取的例子 在爬取动态网页中，我们还需要了解一种异步更新的技术 — AJAX，即“Asynchronous Javascript And XML”（异步JavaScript和XML）。它的价值在于，通过在后台与服务器进行少量数据交换，就可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。一方面减少了网页重复内容的下载，另一方面也节省了流量，因此，AJAX 得到了广泛使用。 相对使用AJAX的网页而言，传统的网页如果需要更新内容，必须重载整个网页页面。因此，AJAX使得互联网应用程序更小、更快，更友好。但是，它的爬虫的过程将变得比较麻烦。 首先，让我们来看看动态网页的例子。我们可以打开我博客的Hello World http://www.santostang.com/2018/07/04/hello-world/ 如下图所示，页面下面的评论就是用JavaScript加载的，这些评论数据不会出现在网页源代码中。  为了验证其用 JavaScript 加载的，我们可以查看此网页的网页源代码。如下如所示，放置该评论的代码，里面并没有评论数据，只有一段JavaScript代码。最后呈现出来的数据就是通过JavaScript提取数据加载到源代码进行呈现的。  除了我的博客，我们还可以在天猫电商上找到AJAX技术的例子。例如，我们打开天猫的iPhone7的产品页面，并点击“累计评价”，我们可以发现上面的url地址没有任何改变，并没有重新加载整个网页，对网页的评论部分进行更新。  和刚刚一样，我们也可以查看此商品网页的源代码，如下如所示，里面并没有用户评论，这一块内容是空白的。  因此，我们如果使用 AJAX 加载的动态网页，怎么爬取里面动态加载的内容呢？有两种方法： 1. 通过浏览器审查元素解析地址 2. 通过selenium模拟浏览器抓取 第四章其他章节请查看 第四章：动态网页抓取 (解析真实地址 + selenium) 4.2 解析真实地址抓取 4.3 通过selenium 模拟浏览器抓取

4.3 通过selenium 模拟浏览器抓取	http://www.santostang.com/2018/07/15/4-3-%e9%80%9a%e8%bf%87selenium-%e6%a8%a1%e6%8b%9f%e6%b5%8f%e8%a7%88%e5%99%a8%e6%8a%93%e5%8f%96/	4.3 通过selenium 模拟浏览器抓取 在上述的例子中，使用Chrome“检查”功能找到源地址还十分容易。但是有一些网站非常复杂，例如前面的天猫产品评论，使用“检查”功能很难找到调用的网页地址。除此之外，有一些数据真实地址的URL也十分冗长和复杂，有些网站为了规避这些抓取会对地址进行加密，造成其中的一些变量让人摸不着头脑。 因此，这里介绍第二种方法，使用浏览器渲染引擎。直接用浏览器在显示网页时解析HTML，应用CSS样式并执行JavaScript的语句。 这方法在爬虫过程中会打开一个浏览器，加载该网页，自动操作浏览器浏览各个网页，顺便把数据抓下来。用一句简单而通俗的话说，使用浏览器渲染方法，爬取动态网页变成了爬取静态网页。 我们可以用Python的selenium库模拟浏览器完成抓取。Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，浏览器自动按照脚本代码做出点击，输入，打开，验证等操作，就像真正的用户在操作一样。 4.3.1 selenium 的安装与基本介绍 selenium的安装非常简单，和其他的Python 库一样，我们可以用pip 安装。 pip install selenium  selenium的脚本可以控制浏览器进行操作，可以实现多个浏览器的调用，包括IE（7, 8, 9, 10, 11），Firefox，Safari，Google Chrome，Opera等。最常用的是 Firefox，因此下面的讲解也以Firefox为例，在执行之前你需要安装Firefox浏览器。 首先，我们打开使用 selenium 打开浏览器和一个网页，以下是代码： from selenium import webdriver driver = webdriver.Firefox() driver.get("http://www.santostang.com/2018/07/04/hello-world/")  运行之后，发现程序报错，错误为： selenium.common.exceptions.WebDriverException:Message: ‘geckodriver’ executable needs to be in PATH. 在selenium之前的版本中，这样做是不会报错的，但是selenium的新版却无法正常运行。于是，我们需要从网上下载一个’geckodriver’，并放在环境变量的PATH中。如果是Windows的话，可以去https://github.com/mozilla/geckodriver/releases 下载最新版的geckodriver，下载是一个zip文件，可以解压后放在Anaconda的安装地址中，例如“C:\ProgramData\Anaconda3\Scripts”。然后在环境变量的PATH中，我们加入这个’geckodriver’的地址。除了下载geckodriver，我们还需要在代码中指定Firefox程序的地址，因此，最后的代码如下： from selenium import webdriver from selenium.webdriver.firefox.firefox_binary import FirefoxBinary   caps = webdriver.DesiredCapabilities().FIREFOX caps["marionette"] = True binary = FirefoxBinary(r'D:\Program Files\Mozilla Firefox\firefox.exe') #把上述地址改成你电脑中Firefox程序的地址 driver = webdriver.Firefox(firefox_binary=binary, capabilities=caps) driver.get("http://www.santostang.com/2018/07/04/hello-world/")  运行后，代码会打开“Hello World”这篇文章的页面。如下图所示：  4.3.2 selenium的实战案例 为了演示Selenium是怎么工作的，我们这里把前面用Chrome“检查”方法获取网页真实地址，然后爬取博客文章评论的网页爬虫方法，写成Selenium的版本。 由于Selenium使用浏览器渲染，因此，那些评论数据已经渲染到了HTML代码中。我们可以使用Chrome“检查”方法，定位到元素位置。 步骤一：找到评论的HTML代码标签。使用Chrome打开该文章页面，右键点击页面，打开“检查”选项。按照第二章的方法，定位到评论数据。如下图所示：可以看到该数据的标签为  第21条测试评论   。  步骤二：尝试获取一条评论数据。在原来打开页面的代码数据上，我们可以使用以下代码，获取第一条评论数据。在下面代码中，driver.find_element_by_css_selector是用CSS选择器查找元素，找到class为’reply-content’的div元素；find_element_by_tag_name则是通过元素的tag去寻找，意思是找到comment中的p元素。最后，再输出p元素中的text文本。 comment = driver.find_element_by_css_selector('div.reply-content') content = comment.find_element_by_tag_name('p') print (content.text)  运行上述代码，我们得到的结果是错误：“Message: Unable to locate element: div.reply-content”。这究竟是为什么呢？ 步骤三：我们可以在 jupyter 中键入driver.page_source 找到为什么没有定位到评论元素，通过排查我们发现，原来代码中的 JavaScript 解析成了一个 iframe， < iframe title=”livere” scrolling=”no”…>也就是说，所有的评论都装在这个框架之中，里面的评论并没有解析出来，所以我们才找不到div.reply-content元素。这时，我们需要加上对 iframe 的解析。 driver.switch_to.frame(driver.find_element_by_css_selector("iframe[title='livere']")) comment = driver.find_element_by_css_selector('div.reply-content') content = comment.find_element_by_tag_name('p') print (content.text)  运行上述代码，我们可以得到最上面的一条评论：“第21条测试评论”。 4.3.3 selenium获取文章的所有评论 上一节我们只是获取了一条评论，如果要获取所有评论，使用循环获取所有评论。具体代码如下： from selenium import webdriver from selenium.webdriver.firefox.firefox_binary  import FirefoxBinary import time  caps = webdriver.DesiredCapabilities().FIREFOX caps["marionette"] = True binary = FirefoxBinary(r'D:\Program Files\Mozilla Firefox\firefox.exe') #把上述地址改成你电脑中Firefox程序的地址  driver = webdriver.Firefox(firefox_binary=binary, capabilities=caps) driver.get("http://www.santostang.com/2018/07/04/hello-world/") driver.switch_to.frame(driver.find_element_by_css_selector("iframe[title='livere']"))   comments = driver.find_elements_by_css_selector('div.reply-content') for eachcomment in comments:     content = eachcomment.find_element_by_tag_name('p')     print (content.text)  代码的前端部分和之前一样，用来打开该文章页面。这里，我们用的是driver.find_elements_by_css_selector(‘div.reply-content’)，在 element 后加了 s，找到所有的评论。然后再用find_element_by_tag_name(‘p’)得到评论中的内容。 运行完成后，打印出来的结果是：  其实，selenium选择元素的方法有很多，具体如下所示：  find_element_by_id：通过元素的id选择，例如:driver.find_element_by_id(‘loginForm’) find_element_by_name：通过元素的name选择，driver.find_element_by_name(‘password’) find_element_by_xpath：通过xpath选择，driver.find_element_by_xpath(“//form[1]”) find_element_by_link_text：通过链接地址选择 find_element_by_partial_link_text：通过链接的部分地址选择 find_element_by_tag_name：通过元素的名称选择 find_element_by_class_name：通过元素的id选择 find_element_by_css_selector：通过css选择器选择  有时候，我们需要查找多个元素。在上述例子中，我们就查找了所有的评论。因此，也有对应的元素选择方法，就是在上述的element后加上s，变成elements。 – find_elements_by_name – find_elements_by_xpath – find_elements_by_link_text – find_elements_by_partial_link_text – find_elements_by_tag_name – find_elements_by_class_name – find_elements_by_css_selector 其中xpath和css_selector是比较好的方法，一方面比较清晰，另一方面相对其他方法定位元素比较准确。 除此之外，我们还可以使用selenium操作元素方法实现自动操作网页。常见的操作元素方法如下： – clear 清除元素的内容 – send_keys 模拟按键输入 – click 点击元素 – submit 提交表单 user = driver.find_element_by_name("username")  #找到用户名输入框 user.clear  #清除用户名输入框内容 user.send_keys("1234567")  #在框中输入用户名 pwd = driver.find_element_by_name("password")  #找到密码输入框 pwd.clear  #清除密码输入框内容 pwd.send_keys("******")    #在框中输入密码 driver.find_element_by_id("loginBtn").click()  #点击登录  上述代码中，是一个自动登录程序中截取的一部分。从代码中可以看到，我们可以用selenium操作元素的方法，对浏览器中的网页进行各种操作，包括登录。 除此之外，selenium除了鼠标简单的操作，还可以实现复杂的双击，拖拽等操作。此外，它还可以获得网页中各个元素的大小，甚至还可以进行模拟键盘的操作。由于篇幅有限，有兴趣的读者，可以到selenium的官方文档查看：http://selenium-python.readthedocs.io/index.html 4.3.4 Selenium的高级操作 使用Selenium和使用浏览器“检查”方法爬取动态网页相比，Selenium因为要把整个网页加载出来，再开始爬取内容，速度往往较慢。 因此在实际使用当中，如果使用浏览器“检查”功能进行网页的逆向工程不复杂的话，最好使用浏览器“检查”功能方法。不过，也有一些方法可以用Selenium控制浏览器的加载的内容，从而加快Selenium的爬取速度。常用的方法有： 1. 控制CSS的加载 2. 控制图片文件的显示 3. 控制JavaScript的执行 1.控制CSS。因为抓取过程中我们仅仅抓取页面的内容，CSS样式文件是用来控制页面的外观和元素放置位置的，对内容并没有影响。因此我们可以限制网页加载CSS，从而减少抓取时间。代码如下所示： from selenium import webdriver from selenium.webdriver.firefox.firefox_binary import FirefoxBinary  caps = webdriver.DesiredCapabilities().FIREFOX caps["marionette"] = True  binary = FirefoxBinary(r'D:\Program Files\Mozilla Firefox\firefox.exe') #把上述地址改成你电脑中Firefox程序的地址 fp = webdriver.FirefoxProfile() fp.set_preference("permissions.default.stylesheet",2)  driver = webdriver.Firefox(firefox_binary=binary, firefox_profile=fp, capabilities=caps) driver.get("http://www.santostang.com/2018/07/04/hello-world/")  运行上述代码，得到的页面如下所示：  2.限制图片的加载。如果我们不需要抓取网页上的图片的话，最好可以禁止图片加载，限制图片的加载可以帮助我们极大地提高网络爬虫的效率。因为网页上的图片往往较多，而且图片文件相对于文字，CSS，JavaScript等其他文件都比较大，所以加载需要较长时间。 from selenium import webdriver from selenium.webdriver.firefox.firefox_binary import FirefoxBinary  caps = webdriver.DesiredCapabilities().FIREFOX caps["marionette"] = False binary = FirefoxBinary(r'D:\Program Files\Mozilla Firefox\firefox.exe') #把上述地址改成你电脑中Firefox程序的地址 fp = webdriver.FirefoxProfile() fp.set_preference("permissions.default.image",2) driver = webdriver.Firefox(firefox_binary=binary, firefox_profile = fp, capabilities=caps) driver.get("http://www.santostang.com/2018/07/04/hello-world/")  运行上述代码，得到的页面如下所示：  3.限制JavaScript的执行。如果我们需要抓取的内容不是通过JavaScript动态加载得到的，我们可以通过禁止JavaScript的执行来提高抓取的效率。因为大多数网页都会利用JavaScript异步地加载很多内容，这些内容不仅是我们不需要的，它们的加载还浪费了时间。 from selenium import webdriver from selenium.webdriver.firefox.firefox_binary import FirefoxBinary  caps = webdriver.DesiredCapabilities().FIREFOX caps["marionette"] = True  binary = FirefoxBinary(r'D:\Program Files\Mozilla Firefox\firefox.exe') #把上述地址改成你电脑中Firefox程序的地址 fp = webdriver.FirefoxProfile() fp.set_preference("javascript.enabled", False) driver = webdriver.Firefox(firefox_binary=binary, firefox_profile = fp, capabilities=caps) driver.get("http://www.santostang.com/2018/07/04/hello-world/")  那么这三种方法，哪一种最节省时间呢？通过对上述三种方法的测试，尝试加载博客的主页50次，并对于加载时间取平均值。我们发现三种方法各自加载所需的时间，如下表所示：  通过上述结果，我们发现 3 种限制方法都能使爬虫加载网页的速度有所加快，其中全部限制对于加载速度的提升效果最好。由于 3 种方法的时间相差并不是很多，再加上网络环境和随机变量的原因，因此我们并不能肯定哪种方法更好。具体的加载速度提升还得看相应的网页，若网页的图片比较多，则限制图片的加载肯定效果很好。 如果能够限制，那么最好限制多种加载，这样的效果最好。 第四章其他章节请查看 第四章：动态网页抓取 (解析真实地址 + selenium) 4.2 解析真实地址抓取 4.3 通过selenium 模拟浏览器抓取

《网络爬虫：从入门到实践》一书勘误	http://www.santostang.com/2018/07/11/%e3%80%8a%e7%bd%91%e7%bb%9c%e7%88%ac%e8%99%ab%ef%bc%9a%e4%bb%8e%e5%85%a5%e9%97%a8%e5%88%b0%e5%ae%9e%e8%b7%b5%e3%80%8b%e4%b8%80%e4%b9%a6%e5%8b%98%e8%af%af/	本书由于是第一版，因此还存在一些差错，希望各位读者谅解。 另外，感谢各位读者的指正，现将本书的错误之处一并放在此处，方便其他读者更好阅读和使用此书。也欢迎大家在知乎私信或者留言给我，我会持续更新此文。 大家可以扫描一下首页左下角的二维码或者在微信搜索 santostang，关注我的公众号“唐松的数据坊”。想加我微信的朋友，可以在公众号发送“加微信”，会自动回复我的微信号。 本书代码的源代码均可在 github下载： https://github.com/Santostang/PythonScraping  。也在百度云下载：https://pan.baidu.com/s/1nuCZbPf 密码请参照本文最末。  Anaconda 的安装(第二章 2.1.1；第9页)：有读者反映Anaconda 链接无法访问，或者下载超过了几个小时。这是因为国内访问境外网站的限速问题。这里推荐使用清华大学的镜像。请选择 Anaconda3下载，最新版本：Anaconda3-5.0.0-Windows-x86.exe 416M 2017-09-26 17:34； Anaconda3-5.0.0-MacOSX-x86_64.pkg 567M 2017-09-26 17:31 编辑器 （第二章，第11页）：应该是编辑器，而非编译器 obj1.detail(obj1) （第二章2.2.5，第18页）：obj1.detail(obj1) 为上一行的接着的注释，意在解释 Python 类的传递，并不是代码。正确的代码是：  class Person:   # 创建类     def __init__(self, name, age): #__init__()方法称为类的构造方法         self.name = name         self.age = age     def detail(self): #通过self调用被封装的内容         print (self.name)         print (self.age) obj1 = Person('santos', 18) obj1.detail()  # Python将obj1传给self参数，即：obj1.detail(obj1)，此时内部self＝obj1   def _ init _(): （第二章2.2.5；第18/19页）：def _ init_ ():应为 def _ _ init _ _(): 单下划线改为双下划线 ，下划线之间没有空格。  class Person:   # 创建类     def __init__(self, name, age): #__init__()方法称为类的构造方法         self.name = name         self.age = age obj1 = Person('santos', 18) #将"santos"和 18 分别封装到 obj1 及 self的 name和age属性   soup.find(“h1″,class_=”post-tilte”) （第二章2.3.2 第23页）：印刷时的 class 后的下划线没印出来，请读者注意，使用 BeautifulSoup 的 find 或 find_all的 class 后均有下划线。 a=I*0.1 （第二章章末实战 第28和29页）：0.01应为0.1。另外29页 elif I<=60 and I>100 应为 elif I<=100 and I>60。 print (movie_list) （第二章3.4.2 第39页）：print (movie_list)应为 print (movies) 第四章：代码已无法使用，因为网易云跟帖停止服务，现在已经在博客中更新了新写的第四章。请参照博客文章。 第四章章末实战（第55页）。Airbnb网站已经改了代码，所以以前的爬虫代码用不了了。代码已经更新在 github和百度网盘。 caps[“marionette”] = True (4.3.2，第48页），caps[“marionette”] = False 这里应该改成caps[“marionette”] = True 反斜杠“\”（第5章5.1.1 第63页），假如你需要匹配文本中的字符”\”，使用编程语言表示的正则表达式里就需要 4 个反斜杠”\\”:前两个反斜杠”\”和后两个反斜杠”\”各自在编程语言里转义成一 个反斜杠”\”，所以 4 个反斜杠”\\”就转义成了两个反斜”\”，这两个反斜杠”\”最 终在正则表达式里转义成一个反斜杠”\”。 第六章章末实战（6.5 第103页）。虎扑网站已经改了代码，所以以前的爬虫代码用不了了。代码已经更新在 github和百度网盘。虎扑的代码也很简单，大家可以自己尝试自己写代码爬虫。   本书更好的使用： 我连 HTML 都不懂，如何自学 HTML？请进入HTML 教程 学习，时间大概2-5小时。 云密码：jhpi

